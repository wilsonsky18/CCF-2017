大家好，我是wilsonlaw，这是我第一次参加天池的比赛，我们队伍最终的成绩是第一赛季60名，第二赛季59名。由于第二赛季基本处于躺尸状态，所以也没有太多的可以分享。因此打算把我们队伍第一赛季的解题思路写下来，留下一点记录，也希望能有更多成绩更好的选手可以开源。

第一赛季有两个版本，多分类和二分类。多分类线上随机森林大概是做到0.9075，二分类做到了0.9189。用的都是随机森林，B榜成绩融合后最终是60名。由于第一次打比赛，初期也走了不少弯路，首先是一直在尝试用多分类的方案，直到A榜最后十多天才转用二分类，其次在看了很多大佬开源的代码后，我觉得我们的特征工程还是做的太粗糙。

拿到赛题以后，最简单的想法就是用经纬度+wifi的定位方式。首先最快速的尝试了一下KNN，经纬度+商城两个特征线上能达到0.7的准确率，由于官方也说经纬度经过脱敏，而且只要略微对经纬度特征进行探索就会发现经纬度存在偏移，在细节方面单纯用经纬度还是有非常大的误差。

经过初步的探索以后就基本确立了wifi为主，经纬度为辅的定位策略，当然也可以自己提取新的信息作为特征。

多分类的方案：
---
提取每个用户搜索到的wif_id作为特征列，对应的强度作为填充值，构建一个高维稀疏矩阵，分不同的mall进行训练。如果这个用户没有搜索到这个wifi，那么对应缺失的强度值用-100填充。所用特征为：经纬度、N个wifi_id，思路和麦芽开源的代码是一致的，这里就不再重复赘述。preprocess.py -> mall_split.py -> random_forest_multi_class.py

每个mall的训练集和测试集都构建相同的稀疏矩阵：

| row_id | wifi_id1| wifi_id2|wifi_id3|wifi_id4|···|wifi_idn|longitude|latitude|shop_id|
| ---- |:-----:| -----:|-----:|-----:|-----:|-----:|-----:|-----:|------:|
| 1  | -100 | -65 |-100 |-42 |···|-100|108.32462|46.23547|s_1043|
| 2 | -100 | -100|-87|-100|···|-64|···|···|s_178|
| 3 | -56| -100|-97|-100|···|-100|···|···|s_458|
| ··· | ···| ···|···|···|···|···|···|···|···|
| m | -47| -100|-100|-73|···|-60|···|···|s_178|

其中-100代表这个用户没有搜到这个wifi_id

二分类的方案：
---

对于二分类，如果不用规则构建候选集，比较方便的方案还是分不同的mall进行训练。针对每一个mall中的每一个shop，使用 OVR 的方案,即将目标shop标记为1，将其他所有shop标记为0。特征工程依然是构建高维稀疏矩阵，我们先将这个高维稀疏矩阵建立本地csv文件，方便后面环节，这一步参考代码binary_data_preprocess.py。得到每个mall的输出文件后，在用random_forest_binary_class.py文件预测。在训练时，只保留正样本出现过的wifi_id，特征的维数下降到几百维，用随机森林对样本进行抽样处理，最后输出预测概率。binary_data_preprocess.py -> random_forest_binary_class.py

同样的，每个mall构建稀疏矩阵，保存到本地文件：

| row_id | wifi_id1| wifi_id2|wifi_id3|wifi_id4|···|wifi_idn|longitude|latitude|shop_id|label|
| ---- |:-----:| -----:|-----:|-----:|-----:|-----:|-----:|-----:|------:|----:|
| 1  | -100 | -65 |-100 |-42 |···|-100|108.32462|46.23547|s_1043|0|
| 2 | -100 | -100|-87|-100|···|-64|···|···|s_178|1|
| 3 | -56| -100|-97|-100|···|-100|···|···|s_458|0|
| ··· | ···| ···|···|···|···|···|···|···|···|···|
| m | -47| -100|-100|-73|···|-60|···|···|s_178|1|

但是在二分类中，对目标shop（这里以s_178为例）中未出现的wiif_id2，这个特征列就可以删除。

| row_id | wifi_id1|wifi_id3|wifi_id4|···|wifi_idn|longitude|latitude|shop_id|label|
| ---- |:-----:| -----:|-----:|-----:|-----:|-----:|-----:|-----:|------:|
| 1  | -100  |-100 |-42 |···|-100|108.32462|46.23547|s_1043|0|
| 2 | -100 |-87|-100|···|-64|···|···|s_178|1|
| 3 | -56| -97|-100|···|-100|···|···|s_458|0|
| ··· | ···| ···|···|···|···|···|···|···|···|
| m | -47|-100|-73|···|-60|···|···|s_178|1|


事实上，数据中还是有非常多的信息可以挖掘，我只进行了一部分的尝试，有些特征因为各种原因并没有进行尝试。

wifi相关特征：
---

1.每个wif_idi的强度均值、方差

2.每个wifi_id被搜索到的次数、被连接的次数。（这两个特征不是越多越好，因为商场自带的wifi其实本身就是一种干扰）

3.每个wifi_id被搜索到次数占每个mall中wifi被搜索次数的比例

用户画像：
---

1.用户历史到某家店的消费次数

2.用户在每一家店消费次数占总消费次数比例


商店相关特征
---
1.每家商店的用户消费次数占商场总消费次数的比例

2.消费次数高的商店之间的相对位置距离、消费次数高和消费次数很低的商店之间的相对位置

3.每家商店被重复光临的次数

还有很多其他的特征需要选手自己挖掘，这里也就不再举例，所谓特征选不好，调参调到老，足以见得特征工程的重要性。比赛中，特征工程应该得到足够的重视

一些反思
---

1.drop_out和gyp开源的代码都使用了时间滑窗法对数据进行更加细致的划分，这一点我们没有想到

2.在多分类上做了太多的尝试，初期没有想到用二分类的方法

3.特征工程尝试过少，因此也没有什么非常强的特征
